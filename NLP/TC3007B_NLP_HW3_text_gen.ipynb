{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "037e89c8",
   "metadata": {},
   "source": [
    "## TC3007B\n",
    "### Text Generation\n",
    "\n",
    "#### Karen Cebreros López - A01704254\n",
    "#### Fermín Méndez García - A01703366\n",
    "#### Emiliano Vásquez Olea - A01707035\n",
    "#### Diego Emilio Barrera Hdz - A01366802\n",
    "#### José Ángel García López - A01275108\n",
    "\n",
    "<br>\n",
    "\n",
    "### Simple LSTM Text Generator using WikiText-2\n",
    "\n",
    "<br>\n",
    "\n",
    "- Objective:\n",
    "    - Gain a fundamental understanding of Long Short-Term Memory (LSTM) networks.\n",
    "    - Develop hands-on experience with sequence data processing and text generation in PyTorch. Given the simplicity of the model, amount of data, and computer resources, the text you generate will not replace ChatGPT, and results must likely will not make a lot of sense. Its only purpose is academic and to understand the text generation using RNNs.\n",
    "    - Enhance code comprehension and documentation skills by commenting on provided starter code.\n",
    "    \n",
    "<br>\n",
    "\n",
    "- Instructions:\n",
    "    - Code Understanding: Begin by thoroughly reading and understanding the code. Comment each section/block of the provided code to demonstrate your understanding. For this, you are encouraged to add cells with experiments to improve your understanding\n",
    "\n",
    "    - Model Overview: The starter code includes an LSTM model setup for sequence data processing. Familiarize yourself with the model architecture and its components. Once you are familiar with the provided model, feel free to change the model to experiment.\n",
    "\n",
    "    - Training Function: Implement a function to train the LSTM model on the WikiText-2 dataset. This function should feed the training data into the model and perform backpropagation. \n",
    "\n",
    "    - Text Generation Function: Create a function that accepts starting text (seed text) and a specified total number of words to generate. The function should use the trained model to generate a continuation of the input text.\n",
    "\n",
    "    - Code Commenting: Ensure that all the provided starter code is well-commented. Explain the purpose and functionality of each section, indicating your understanding.\n",
    "\n",
    "    - Submission: Submit your Jupyter Notebook with all sections completed and commented. Include a markdown cell with the full names of all contributing team members at the beginning of the notebook.\n",
    "    \n",
    "<br>\n",
    "\n",
    "- Evaluation Criteria:\n",
    "    - Code Commenting (60%): The clarity, accuracy, and thoroughness of comments explaining the provided code. You are suggested to use markdown cells for your explanations.\n",
    "\n",
    "    - Training Function Implementation (20%): The correct implementation of the training function, which should effectively train the model.\n",
    "\n",
    "    - Text Generation Functionality (10%): A working function is provided in comments. You are free to use it as long as you make sure to uderstand it, you may as well improve it as you see fit. The minimum expected is to provide comments for the given function. \n",
    "\n",
    "    - Conclusions (10%): Provide some final remarks specifying the differences you notice between this model and the one used  for classification tasks. Also comment on changes you made to the model, hyperparameters, and any other information you consider relevant. Also, please provide 3 examples of generated texts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb4b117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karencl/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import portalocker\n",
    "\n",
    "#PyTorch libraries\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import WikiText2\n",
    "# Dataloader library\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "# Libraries to prepare the data\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "# neural layers\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d8ff971",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3288ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos los datos\n",
    "train_dataset, val_dataset, test_dataset = WikiText2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc4c7dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos los tokens\n",
    "tokeniser = get_tokenizer('basic_english')\n",
    "\n",
    "def yield_tokens(data):\n",
    "    for text in data:\n",
    "        yield tokeniser(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c2cb068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el vocabulario\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_dataset), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "\n",
    "# Ponemos el 'unknown token' en la posición 0\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "134b832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 50\n",
    "def data_process(raw_text_iter, seq_length = 50):\n",
    "    ''' Función que procesa los datos\n",
    "    Args:\n",
    "        raw_text_iter - dataset\n",
    "        seq_length - tamaño de la secuencia\n",
    "    Return:\n",
    "        tensores\n",
    "    '''\n",
    "    data = [torch.tensor(vocab(tokeniser(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "\n",
    "    # Quitamos los tensores vacíos\n",
    "    data = torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "    # Quitamos tokens adicionales\n",
    "    return (data[:-(data.size(0)%seq_length)].view(-1, seq_length), \n",
    "            data[1:-(data.size(0)%seq_length-1)].view(-1, seq_length))  \n",
    "\n",
    "\n",
    "# Creamos los tensores para los conjuntos de datos (x -> data, y -> label)\n",
    "x_train, y_train = data_process(train_dataset, seq_length)\n",
    "x_val, y_val = data_process(val_dataset, seq_length)\n",
    "x_test, y_test = data_process(test_dataset, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b54c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos en tensores para poder pasarlos al DataLoader\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "test_dataset = TensorDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4d400fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Hacemos los DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "59c63b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la clase del modelo LSTM (como el de la clase)\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Capa de embedding\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Capa LSTM\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        # Capa lineal (fully connected)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, text, hidden):\n",
    "        # Se sacan los embeddings\n",
    "        embeddings = self.embeddings(text)\n",
    "        # Se obtiene el output de LSTM y la nueva capa oculta\n",
    "        output, hidden = self.lstm(embeddings, hidden)\n",
    "        # Se pasa el output de LSTM a una capa lineal (fully connected)\n",
    "        decoded = self.fc(output)\n",
    "        return decoded, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Se devuelven los tensores (estado oculto)\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))\n",
    "\n",
    "\n",
    "vocab_size = len(vocab) # Tamaño del vocabulario\n",
    "emb_size = 100 # Tamaño del los embeddings\n",
    "neurons = 256 # Número de neuronas\n",
    "num_layers = 1 # Cantidad de capas nn.LSTM \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "215eabb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, optimiser, criterion):\n",
    "    ''' Función que entrena al modelo\n",
    "    Args:\n",
    "        model - modelo LSTM\n",
    "        epochs - número de épocas\n",
    "        optimiser - optimiser (Adam)\n",
    "    '''\n",
    "    model = model.to(device=device)\n",
    "    model.train()\n",
    "    \n",
    "    # Se itera sobre el número de épocas\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch: {epoch}')\n",
    "        # Se itera sobre los batches de train_loader\n",
    "        for i, (data, targets) in enumerate((train_loader)):\n",
    "            # Reset the gradient\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            data = data.to(device=device, dtype=torch.long)\n",
    "            targets = targets.to(device=device, dtype=torch.long)\n",
    "\n",
    "            # Se obtiene el tamaño del batch actual, se inicializa el estado oculto y se corre el modelo\n",
    "            batch_size = data.size(0)\n",
    "            hidden = model.init_hidden(batch_size)\n",
    "            output, hidden = model(data, hidden)\n",
    "\n",
    "            # Se calcula la pérdida\n",
    "            loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "            # Se hace retropropagación para el cálculo de los gradientes\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 0.5) \n",
    "\n",
    "            # Se actualizan los pesos \n",
    "            optimiser.step()\n",
    "\n",
    "            if (i % 100 == 0):\n",
    "                print(f'\\t Batch: {i}, Loss: {loss.item()}')             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "aa9c84ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "\t Batch: 0, Loss: 10.263138771057129\n",
      "\t Batch: 100, Loss: 6.364813327789307\n",
      "\t Batch: 200, Loss: 6.044144153594971\n",
      "\t Batch: 300, Loss: 5.80185604095459\n",
      "\t Batch: 400, Loss: 5.775587558746338\n",
      "\t Batch: 500, Loss: 5.660407066345215\n",
      "\t Batch: 600, Loss: 5.504097938537598\n",
      "Epoch: 1\n",
      "\t Batch: 0, Loss: 5.320652008056641\n",
      "\t Batch: 100, Loss: 5.291232109069824\n",
      "\t Batch: 200, Loss: 5.267204761505127\n",
      "\t Batch: 300, Loss: 5.216509819030762\n",
      "\t Batch: 400, Loss: 5.241329193115234\n",
      "\t Batch: 500, Loss: 5.133429050445557\n",
      "\t Batch: 600, Loss: 5.167105674743652\n",
      "Epoch: 2\n",
      "\t Batch: 0, Loss: 4.767019271850586\n",
      "\t Batch: 100, Loss: 4.762258529663086\n",
      "\t Batch: 200, Loss: 4.876164436340332\n",
      "\t Batch: 300, Loss: 4.735452175140381\n",
      "\t Batch: 400, Loss: 4.822271347045898\n",
      "\t Batch: 500, Loss: 4.884099006652832\n",
      "\t Batch: 600, Loss: 4.801356315612793\n",
      "Epoch: 3\n",
      "\t Batch: 0, Loss: 4.42833137512207\n",
      "\t Batch: 100, Loss: 4.481601238250732\n",
      "\t Batch: 200, Loss: 4.542435169219971\n",
      "\t Batch: 300, Loss: 4.466656684875488\n",
      "\t Batch: 400, Loss: 4.483693599700928\n",
      "\t Batch: 500, Loss: 4.455982685089111\n",
      "\t Batch: 600, Loss: 4.643041610717773\n",
      "Epoch: 4\n",
      "\t Batch: 0, Loss: 4.139338493347168\n",
      "\t Batch: 100, Loss: 4.209019660949707\n",
      "\t Batch: 200, Loss: 4.230296611785889\n",
      "\t Batch: 300, Loss: 4.363221645355225\n",
      "\t Batch: 400, Loss: 4.273610591888428\n",
      "\t Batch: 500, Loss: 4.322656154632568\n",
      "\t Batch: 600, Loss: 4.366199970245361\n"
     ]
    }
   ],
   "source": [
    "# Creamos el modelo\n",
    "model = LSTMModel(vocab_size, emb_size, neurons, num_layers)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Función de pérdida\n",
    "lr = 0.005 # Learning rate\n",
    "epochs = 5 # Número de épocas\n",
    "optimiser = optim.Adam(model.parameters(), lr=lr) # optimiser\n",
    "\n",
    "# Se llama a la función \"train\" para entrenar al modelo\n",
    "train(model, epochs, optimiser, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c8667411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_text, num_words, temperature=1.0):\n",
    "    ''' Función que genera texto a partir de un \"inicio\" dado\n",
    "    Args:\n",
    "        model - nuestro modelo ya entrenado\n",
    "        start_text - el inicio del texto\n",
    "        num_words - cantidad de palabras deseadas en el texto final\n",
    "        temperature - aleatoriedad de las predicciones\n",
    "    Return:\n",
    "        texto generado completo\n",
    "    '''\n",
    "    model.eval()\n",
    "    words = tokeniser(start_text)\n",
    "    hidden = model.init_hidden(1)\n",
    "\n",
    "    # Se hace el ciclo hasta generar la cantidad de palabras deseadas\n",
    "    for i in range(0, num_words):\n",
    "        x_indices = [vocab[word] for word in words[i:]] # Índices de las palabras en la secuencia (de principio a fin)\n",
    "        x = torch.tensor([x_indices], device=device, dtype=torch.long)\n",
    "        \n",
    "        # Se genera la siguiente palabra\n",
    "        y_pred, hidden = model(x, hidden)\n",
    "        \n",
    "        # Se obtienen los scores, se convierten en probabilidades usando 'softmax'\n",
    "        scores = y_pred[0][-1]\n",
    "        p = (F.softmax(scores / temperature, dim=0).detach()).to(device='cpu').numpy()\n",
    "\n",
    "        # Se elige la nueva palabra con dichas propapbilidades y se agrega a la lista de palabras\n",
    "        word_index = np.random.choice(len(scores), p=p)\n",
    "        words.append(vocab.lookup_token(word_index))\n",
    "\n",
    "    # Convierte la lista de palabras a un texto completo\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like you , aweary i know\n",
      "i wish i had a different view . (\n",
      "i would like to the along . <unk> in <unk> <unk> , here henry\n"
     ]
    }
   ],
   "source": [
    "# Se manda a llamar \"generate_text\", para generar un texto de 50 palabras, empezando por 'I like'\n",
    "print(generate_text(model, start_text=\"I like\", num_words=5))\n",
    "\n",
    "# Se manda a llamar \"generate_text\", para generar un texto de 20 palabras, empezando por 'I like'\n",
    "print(generate_text(model, start_text=\"I wish I had\", num_words=5))\n",
    "\n",
    "# Se manda a llamar \"generate_text\", para generar un texto de 50 palabras, empezando por 'I like'\n",
    "print(generate_text(model, start_text=\"I would like to\", num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusión:\n",
    "\n",
    "A diferencia del modelo creado en la segunda actividad (el clasificador de texto), este modelo tardó más en entrenar. Es más complejo ya que el otro solo clasificaba lo que se le pasa y este lo genera; por lo que es más pesado, más dificl de tunear y como dije, tarda más en entrenarse.\n",
    "\n",
    "Este modelo es igual al que hicimos en la clase, a excepción del learning rate que es un poco más alto. Podemos ver que solo está un pedazo del entrenamiento, con el cual en sí las generaciones de texto hechas no son muy buenas. Sin embargo, creemos que si se dejara corriendo durante más épocas, o si se jugara un poquito con la estructura del modelo, tal vez el accuracy aumentaría y el texto generado sería mejor. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
